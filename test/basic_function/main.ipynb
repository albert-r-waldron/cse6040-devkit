{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": [
     "main.header"
    ]
   },
   "source": [
    "# `Coffee Problem \"Fizzbuzz\"`: `assignment subtitle`\n",
    "_Version 0.0.1_\n",
    "\n",
    "*All of the header information is important. Please read it..*\n",
    "\n",
    "**Topics number of exercises:** This problem builds on your knowledge of `this, that, and the other`. It has **3** exercises numbered 0 to **2**. There are **total points** available points. However to earn 100% the threshold is **points cap** points. (Therefore once you hit **points cap** points you can stop. There is no extra credit for exceeding this threshold.)\n",
    "\n",
    "**Exercise ordering:** Each exercise builds logically on previous exercises but you may solve them in any order. That is if you can't solve an exercise you can still move on and try the next one. Use this to your advantage as the exercises are **not** necessarily ordered in terms of difficulty. Higher point values generally indicate more difficult exercises. \n",
    "\n",
    "**Demo cells:** Code cells starting with the comment `### Run Me!!!` load results from prior exercises applied to the entire data set and use those to build demo inputs. These must be run for subsequent demos to work properly but they do not affect the test cells. The data loaded in these cells may be rather large (at least in terms of human readability). You are free to print or otherwise use Python to explore them but we may not print them in the starter code.\n",
    "\n",
    "**Debugging your code:** Right before each exercise test cell there is a block of text explaining the variables available to you for debugging. You may use these to test your code and can print/display them as needed (careful when printing large objects you may want to print the head or chunks of rows at a time).\n",
    "\n",
    "**Exercise point breakdown:**\n",
    "\n",
    "\n",
    "- Exercise 0 - : **1** point(s)\n",
    "\n",
    "- Exercise 1 - : **1** point(s)\n",
    "\n",
    "- Exercise 2 - : **1** point(s)\n",
    "\n",
    "\n",
    "**Final reminders:** \n",
    "\n",
    "- Submit after **every exercise**\n",
    "- Review the generated grade report after you submit to see what errors were returned\n",
    "- Stay calm, skip problems as needed and take short breaks at your leisure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "main.global_imports"
    ]
   },
   "outputs": [],
   "source": [
    "### Global imports\n",
    "import dill\n",
    "from cse6040_devkit import plugins, utils\n",
    "import re \n",
    "import pandas as pd\n",
    "\n",
    "utils.add_from_file('convert_to_str', plugins)\n",
    "utils.add_from_file('foo', utils)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": [
     "fizzbuzz.prompt"
    ]
   },
   "source": [
    "### Exercise 0: (1 points)\n",
    "**fizzbuzz**  \n",
    "\n",
    "**Your task:** define `fizzbuzz_soln` as follows:\n",
    "\n",
    "`fizzbuzz_soln` solves the classic \"fizzbuzz\" problem for a single integer. \n",
    "\n",
    "**Inputs**:\n",
    "- `i` (int): Any integer\n",
    "\n",
    "**Returns**: one of the following depending on whether 3, 5, both or neither divide evenly into `i`\n",
    "- `'Fizz'`     if i is divisible by 3                Example: 9\n",
    "- `'Buzz'`     if i is divisible by 5                Example: 35\n",
    "- `'FizzBuzz'` if i is divisible by both 3 and 5     Example: 45\n",
    "- `i`          if i is divisible by neither 3 nor 5  Example: 22\n",
    "\n",
    "---\n",
    "We have defined a **helper function**, `x_divides_y` as follows:  \n",
    "\n",
    "`x_divides_y` evaluates whether one integer (`x`) divides evenly into another integer (`y`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "fizzbuzz.solution"
    ]
   },
   "outputs": [],
   "source": [
    "### Helper Function\n",
    "def x_divides_y(x: int, y: int):\n",
    "    return y%x == 0\n",
    "\n",
    "### Solution - Exercise 0  \n",
    "def fizzbuzz_soln(z: int):\n",
    "    ### BEGIN SOLUTION\n",
    "    return (\"Fizz\"*x_divides_y(3, z)+\"Buzz\"*x_divides_y(5, z)) or z\n",
    "    ### END SOLUTION\n",
    "\n",
    "### Demo function call\n",
    "for x in [9, 35, 45, 22]:\n",
    "    print(f'fizzbuzz_soln({x}) -> {fizzbuzz_soln(x)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": [
     "fizzbuzz.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this printed output.**\n",
    "```\n",
    "fizzbuzz_soln(9) -> Fizz\n",
    "fizzbuzz_soln(35) -> Buzz\n",
    "fizzbuzz_soln(45) -> FizzBuzz\n",
    "fizzbuzz_soln(22) -> 22\n",
    "```\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for fizzbuzz (exercise 0). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_0",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "fizzbuzz.test"
    ]
   },
   "outputs": [],
   "source": [
    "### Test Cell - Exercise 0  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars, e = execute_tests(func=plugins.convert_to_str(fizzbuzz_soln),\n",
    "              ex_name='fizzbuzz',\n",
    "              key=b'm_8VEe8rANbW9jOWIO0Rnu4v-gwlfczi_3yl_z0lBIo=', \n",
    "              n_iter=100)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "if e: raise e\n",
    "assert passed, 'The solution to fizzbuzz did not pass the test.'\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "passed, test_case_vars, e = execute_tests(func=plugins.convert_to_str(fizzbuzz_soln),\n",
    "              ex_name='fizzbuzz',\n",
    "              key=b'Mm4d7jm3-w_0wdLOp-1GLFB1IPLKtaq_apcVh4y-Xfg=', \n",
    "              n_iter=1,\n",
    "              hidden=True)\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "if e: raise e\n",
    "assert passed, 'The solution to fizzbuzz did not pass the test.'\n",
    "### END HIDDEN TESTS\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": [
     "freebie.preload_objects"
    ]
   },
   "outputs": [],
   "source": [
    "### Run Me!!!\n",
    "free_data = utils.load_object_from_publicdata('free_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "tags": [
     "freebie.prompt"
    ]
   },
   "source": [
    "### Exercise 1: (1 points)\n",
    "**freebie**  \n",
    "\n",
    "**Example:** we have defined `freebie` as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": [
     "freebie.solution"
    ]
   },
   "outputs": [],
   "source": [
    "### Solution - Exercise 1  \n",
    "def freebie():\n",
    "    ### This will never be tested\n",
    "    \n",
    "\n",
    "    print(free_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "tags": [
     "freebie.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    " <!-- Test Cell Boilerplate -->  \n",
    " The test cell below will always pass. Please submit to collect your free points for freebie (exercise 1).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_1",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "freebie.test"
    ]
   },
   "outputs": [],
   "source": [
    "### Test Cell - Exercise 1  \n",
    "\n",
    "\n",
    "print('Passed! Please submit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": [
     "species_count.preload_objects"
    ]
   },
   "outputs": [],
   "source": [
    "### Run Me!!!\n",
    "char_df = utils.load_object_from_publicdata('char_df')\n",
    "demo_result_species_count_TRUE = utils.load_object_from_publicdata('demo_result_species_count_TRUE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "tags": [
     "species_count.prompt"
    ]
   },
   "source": [
    "### Exercise 2: (1 points)\n",
    "**species_count**  \n",
    "\n",
    "**Your task:** define `species_count_query` as follows:\n",
    "\n",
    "The query should return these columns\n",
    "- species (string)\n",
    "- count (int): count of species value in characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": [
     "species_count.solution"
    ]
   },
   "outputs": [],
   "source": [
    "### Solution - Exercise 2  \n",
    "\n",
    "species_count_query = '''YOUR QUERY HERE'''\n",
    "### BEGIN SOLUTION\n",
    "species_count_query = '''\n",
    "                      select species, count(1) as \"count\"\n",
    "                      from characters\n",
    "                      group by species\n",
    "                  '''\n",
    "### END SOLUTION\n",
    "\n",
    "### Demo function call\n",
    "from cse6040_devkit.tester_fw.test_utils \\\n",
    "    import dfs_to_conn\n",
    "conn = dfs_to_conn({'characters': char_df})\n",
    "demo_result_species_count = \\\n",
    "    plugins.sql_executor(species_count_query)(conn)\n",
    "display(demo_result_species_count.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "tags": [
     "species_count.test_boilerplate"
    ]
   },
   "source": [
    " \n",
    "\n",
    "**The demo should display this output.**  \n",
    "\n",
    "|    | species     |   count |\n",
    "|---:|:------------|--------:|\n",
    "|  0 | Ardennian   |       1 |\n",
    "|  1 | Besalisk    |       1 |\n",
    "|  2 | Chiss       |       2 |\n",
    "|  3 | Clawdite    |       1 |\n",
    "|  4 | Dathomirian |       3 |\n",
    "\n",
    "\n",
    " ---\n",
    " <!-- Test Cell Boilerplate -->  \n",
    "The cell below will test your solution for species_count (exercise 2). The testing variables will be available for debugging under the following names in a dictionary format.  \n",
    "- `input_vars` - Input variables for your solution.   \n",
    "- `original_input_vars` - Copy of input variables from prior to running your solution. Any `key:value` pair in `original_input_vars` should also exist in `input_vars` - otherwise the inputs were modified by your solution.  \n",
    "- `returned_output_vars` - Outputs returned by your solution.  \n",
    "- `true_output_vars` - The expected output. This _should_ \"match\" `returned_output_vars` based on the question requirements - otherwise, your solution is not returning the correct output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "ex_2",
     "locked": true,
     "points": 1,
     "solution": false
    },
    "tags": [
     "species_count.test"
    ]
   },
   "outputs": [],
   "source": [
    "### Test Cell - Exercise 2  \n",
    "\n",
    "# Load testing utility\n",
    "with open('resource/asnlib/publicdata/execute_tests', 'rb') as f:\n",
    "    execute_tests = dill.load(f)\n",
    "\n",
    "# Execute test\n",
    "passed, test_case_vars, e = execute_tests(func=plugins.sql_executor(species_count_query),\n",
    "              ex_name='species_count',\n",
    "              key=b'm_8VEe8rANbW9jOWIO0Rnu4v-gwlfczi_3yl_z0lBIo=', \n",
    "              n_iter=100)\n",
    "# Assign test case vars for debugging\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "if e: raise e\n",
    "assert passed, 'The solution to species_count did not pass the test.'\n",
    "\n",
    "### BEGIN HIDDEN TESTS\n",
    "passed, test_case_vars, e = execute_tests(func=plugins.sql_executor(species_count_query),\n",
    "              ex_name='species_count',\n",
    "              key=b'Mm4d7jm3-w_0wdLOp-1GLFB1IPLKtaq_apcVh4y-Xfg=', \n",
    "              n_iter=1,\n",
    "              hidden=True)\n",
    "input_vars, original_input_vars, returned_output_vars, true_output_vars = test_case_vars\n",
    "if e: raise e\n",
    "assert passed, 'The solution to species_count did not pass the test.'\n",
    "### END HIDDEN TESTS\n",
    "print('Passed! Please submit.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
